{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Batch Normalization in Artificial Neural Networks (ANN)"
      ],
      "metadata": {
        "id": "zME6ecmzlvGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the concept of batch normalization in the context of Artificial Neural Networks:\n",
        "\n",
        "- Batch normalization (BN) is a technique to improve the training speed and performance of neural networks. It normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
        "\n",
        "2. Describe the benefits of using batch normalization during training:\n",
        "\n",
        "- Faster Training: BN allows for higher learning rates by reducing internal covariate shift.\n",
        "- Stability: It stabilizes the training process, reducing the sensitivity to initialization.\n",
        "- Regularization: BN acts as a form of regularization, potentially reducing the need for other regularization techniques like dropout.\n",
        "\n",
        "3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters:\n",
        "- Normalization Step: For each mini-batch, calculate the mean and variance of the activations. Normalize each activation by subtracting the mean and dividing by the standard deviation.\n",
        "- Learnable Parameters: After normalization, the output is scaled and shifted using learnable parameters (gamma and beta), allowing the network to maintain the representational power."
      ],
      "metadata": {
        "id": "euKYQ2lYlw9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Implementation:"
      ],
      "metadata": {
        "id": "JRhEQWIDquvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Choose a dataset of your choice (e.g., MNIST, CIFAR-10) and preprocess it:"
      ],
      "metadata": {
        "id": "ST8WRR_omV-L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jBUc7u5MlU-3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g., TensorFlow, PyTorch):"
      ],
      "metadata": {
        "id": "m2-Ep2GtoQBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "gcjB8ossmwwt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Train the neural network on the chosen dataset without using batch normalization:"
      ],
      "metadata": {
        "id": "8YH8nggTq-gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "train(model, train_loader, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J9GHM9OnB5k",
        "outputId": "ccc3cb04-32a3-4053-c985-e070d0fbb90d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.40141680640484223\n",
            "Epoch 2, Loss: 0.1661402302514166\n",
            "Epoch 3, Loss: 0.11830560415625763\n",
            "Epoch 4, Loss: 0.09260945883529909\n",
            "Epoch 5, Loss: 0.07818306103618795\n",
            "Epoch 6, Loss: 0.06611174414964961\n",
            "Epoch 7, Loss: 0.05378639259894909\n",
            "Epoch 8, Loss: 0.04733584208231765\n",
            "Epoch 9, Loss: 0.04049042546815092\n",
            "Epoch 10, Loss: 0.03641706440918722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implement batch normalization layers in the neural network and train the model again:"
      ],
      "metadata": {
        "id": "fPdzqFZ4q_-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNNWithBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNNWithBN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model_bn = SimpleNNWithBN()\n",
        "optimizer_bn = optim.SGD(model_bn.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "DdpINAO3rB45"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the modified model:\n",
        "train(model_bn, train_loader, criterion, optimizer_bn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqSNY2jzr0x4",
        "outputId": "34d72a9a-1079-4957-ba82-c3d6522b6310"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.21599571231399167\n",
            "Epoch 2, Loss: 0.08668426302537671\n",
            "Epoch 3, Loss: 0.06035005617519416\n",
            "Epoch 4, Loss: 0.04373052048579808\n",
            "Epoch 5, Loss: 0.03365582184814441\n",
            "Epoch 6, Loss: 0.026780540008159447\n",
            "Epoch 7, Loss: 0.022409169710174735\n",
            "Epoch 8, Loss: 0.016622860357897586\n",
            "Epoch 9, Loss: 0.016902302078712707\n",
            "Epoch 10, Loss: 0.012140795331523279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization:"
      ],
      "metadata": {
        "id": "Tl85yJdTrDC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "accuracy = evaluate(model, test_loader)\n",
        "accuracy_bn = evaluate(model_bn, test_loader)\n",
        "\n",
        "print(f'Accuracy without BN: {accuracy}%')\n",
        "print(f'Accuracy with BN: {accuracy_bn}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLrwDGm6rGOm",
        "outputId": "6f8d7fa2-77ae-4a25-9bd5-0b5aa4442194"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without BN: 97.76%\n",
            "Accuracy with BN: 98.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Discuss the impact of batch normalization on the training process and the performance of the neural network:\n",
        "\n",
        "- Training Speed: Batch normalization can help the model converge faster by allowing higher learning rates.\n",
        "- Stability: It reduces the internal covariate shift, stabilizing the learning process.\n",
        "- Performance: Batch normalization generally improves the model's performance, resulting in better accuracy on both training and validation sets."
      ],
      "metadata": {
        "id": "yK-lxDo8rIbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Neural network without batch normalization\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "train(model, train_loader, criterion, optimizer)\n",
        "\n",
        "# Neural network with batch normalization\n",
        "class SimpleNNWithBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNNWithBN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model_bn = SimpleNNWithBN()\n",
        "optimizer_bn = optim.SGD(model_bn.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "train(model_bn, train_loader, criterion, optimizer_bn)\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "accuracy = evaluate(model, test_loader)\n",
        "accuracy_bn = evaluate(model_bn, test_loader)\n",
        "\n",
        "print(f'Accuracy without BN: {accuracy}%')\n",
        "print(f'Accuracy with BN: {accuracy_bn}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZwITvDKrI4w",
        "outputId": "86a6e627-f676-4bbd-cb77-5444e942405c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4043550709608013\n",
            "Epoch 2, Loss: 0.16365424480074758\n",
            "Epoch 3, Loss: 0.11631351864056737\n",
            "Epoch 4, Loss: 0.08867974621680642\n",
            "Epoch 5, Loss: 0.07432134291862072\n",
            "Epoch 6, Loss: 0.0644305779780847\n",
            "Epoch 7, Loss: 0.05525515688536589\n",
            "Epoch 8, Loss: 0.04605809748974512\n",
            "Epoch 9, Loss: 0.03988742701913307\n",
            "Epoch 10, Loss: 0.035847287947140226\n",
            "Epoch 1, Loss: 0.21517041752905225\n",
            "Epoch 2, Loss: 0.08624984715074333\n",
            "Epoch 3, Loss: 0.0592294706400039\n",
            "Epoch 4, Loss: 0.04570024191532959\n",
            "Epoch 5, Loss: 0.03304272288367696\n",
            "Epoch 6, Loss: 0.027324564059077305\n",
            "Epoch 7, Loss: 0.0239504706890219\n",
            "Epoch 8, Loss: 0.018043148601438845\n",
            "Epoch 9, Loss: 0.015576080385179046\n",
            "Epoch 10, Loss: 0.013250092071776928\n",
            "Accuracy without BN: 97.61%\n",
            "Accuracy with BN: 98.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Experimentation and Analysis:\n",
        "1. Experiment with different batch size and observe the effect on the training dynamics and model performance.\n"
      ],
      "metadata": {
        "id": "NnIARGhMt1NR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To observe the effect of different batch sizes on training dynamics and model performance, we can modify the batch size in our data loaders and retrain the models. Let's choose three different batch sizes: 32, 64, and 128."
      ],
      "metadata": {
        "id": "t38-zon-u53q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Size 32:\n",
        "train_loader_32 = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader_32 = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Train model without batch normalization\n",
        "model_32 = SimpleNN()\n",
        "optimizer_32 = optim.SGD(model_32.parameters(), lr=0.01, momentum=0.9)\n",
        "train(model_32, train_loader_32, criterion, optimizer_32)\n",
        "\n",
        "# Train model with batch normalization\n",
        "model_bn_32 = SimpleNNWithBN()\n",
        "optimizer_bn_32 = optim.SGD(model_bn_32.parameters(), lr=0.01, momentum=0.9)\n",
        "train(model_bn_32, train_loader_32, criterion, optimizer_bn_32)\n",
        "\n",
        "# Evaluate both models\n",
        "accuracy_32 = evaluate(model_32, test_loader_32)\n",
        "accuracy_bn_32 = evaluate(model_bn_32, test_loader_32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-wVl926uRf0",
        "outputId": "d073b80a-0d78-4ef7-f3ba-955f3bcb62fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.3310694493830204\n",
            "Epoch 2, Loss: 0.14309563914984463\n",
            "Epoch 3, Loss: 0.10695876042315115\n",
            "Epoch 4, Loss: 0.08459196136618653\n",
            "Epoch 5, Loss: 0.07003962687837581\n",
            "Epoch 6, Loss: 0.0571495078012192\n",
            "Epoch 7, Loss: 0.04966393201490088\n",
            "Epoch 8, Loss: 0.04304751966406475\n",
            "Epoch 9, Loss: 0.04027762156855703\n",
            "Epoch 10, Loss: 0.03384864194946131\n",
            "Epoch 1, Loss: 0.20819080470651388\n",
            "Epoch 2, Loss: 0.09568213318772614\n",
            "Epoch 3, Loss: 0.07200215510924657\n",
            "Epoch 4, Loss: 0.05621409612915789\n",
            "Epoch 5, Loss: 0.04549729872699827\n",
            "Epoch 6, Loss: 0.035941357943741606\n",
            "Epoch 7, Loss: 0.03143149889875203\n",
            "Epoch 8, Loss: 0.02614082324236321\n",
            "Epoch 9, Loss: 0.021808457904080085\n",
            "Epoch 10, Loss: 0.01920710031868269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Size 64:\n",
        "train_loader_64 = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader_64 = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Train model without batch normalization\n",
        "model_64 = SimpleNN()\n",
        "optimizer_64 = optim.SGD(model_64.parameters(), lr=0.01, momentum=0.9)\n",
        "train(model_64, train_loader_64, criterion, optimizer_64)\n",
        "\n",
        "# Train model with batch normalization\n",
        "model_bn_64 = SimpleNNWithBN()\n",
        "optimizer_bn_64 = optim.SGD(model_bn_64.parameters(), lr=0.01, momentum=0.9)\n",
        "train(model_bn_64, train_loader_64, criterion, optimizer_bn_64)\n",
        "\n",
        "# Evaluate both models\n",
        "accuracy_64 = evaluate(model_64, test_loader_64)\n",
        "accuracy_bn_64 = evaluate(model_bn_64, test_loader_64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbgb5yhnuUjo",
        "outputId": "945329df-4294-4f5e-bf18-ac3d3df7a45a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4077172031812767\n",
            "Epoch 2, Loss: 0.16736758740614854\n",
            "Epoch 3, Loss: 0.11737099300915085\n",
            "Epoch 4, Loss: 0.09479700793116999\n",
            "Epoch 5, Loss: 0.07708533286555871\n",
            "Epoch 6, Loss: 0.06422849480675331\n",
            "Epoch 7, Loss: 0.05640166123081912\n",
            "Epoch 8, Loss: 0.04968486795053462\n",
            "Epoch 9, Loss: 0.04060087869283128\n",
            "Epoch 10, Loss: 0.034686552515791565\n",
            "Epoch 1, Loss: 0.21563254157776263\n",
            "Epoch 2, Loss: 0.0870849877717033\n",
            "Epoch 3, Loss: 0.059885570633489246\n",
            "Epoch 4, Loss: 0.044925752793625394\n",
            "Epoch 5, Loss: 0.03389470920419452\n",
            "Epoch 6, Loss: 0.025387144549243422\n",
            "Epoch 7, Loss: 0.022432971158254045\n",
            "Epoch 8, Loss: 0.01845445458672748\n",
            "Epoch 9, Loss: 0.016028627575252766\n",
            "Epoch 10, Loss: 0.013997254320012002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Size 128:\n",
        "train_loader_128 = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader_128 = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Train model without batch normalization\n",
        "model_128 = SimpleNN()\n",
        "optimizer_128 = optim.SGD(model_128.parameters(), lr=0.01, momentum=0.9)\n",
        "train(model_128, train_loader_128, criterion, optimizer_128)\n",
        "\n",
        "# Train model with batch normalization\n",
        "model_bn_128 = SimpleNNWithBN()\n",
        "optimizer_bn_128 = optim.SGD(model_bn_128.parameters(), lr=0.01, momentum=0.9)\n",
        "train(model_bn_128, train_loader_128, criterion, optimizer_bn_128)\n",
        "\n",
        "# Evaluate both models\n",
        "accuracy_128 = evaluate(model_128, test_loader_128)\n",
        "accuracy_bn_128 = evaluate(model_bn_128, test_loader_128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YylXwYFVuUgL",
        "outputId": "84fd0d87-1549-4613-b735-577a02c51184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.5423439370988529\n",
            "Epoch 2, Loss: 0.23217710630217595\n",
            "Epoch 3, Loss: 0.16505739236596043\n",
            "Epoch 4, Loss: 0.12499488233280842\n",
            "Epoch 5, Loss: 0.10433103207713251\n",
            "Epoch 6, Loss: 0.08848691769817998\n",
            "Epoch 7, Loss: 0.07627236963843485\n",
            "Epoch 8, Loss: 0.06651358935894615\n",
            "Epoch 9, Loss: 0.058390087614864555\n",
            "Epoch 10, Loss: 0.05276924473787549\n",
            "Epoch 1, Loss: 0.26144389499193316\n",
            "Epoch 2, Loss: 0.0904197796686753\n",
            "Epoch 3, Loss: 0.05856420552886244\n",
            "Epoch 4, Loss: 0.041727523343252346\n",
            "Epoch 5, Loss: 0.031766765220249606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print out the results for easy comparison:"
      ],
      "metadata": {
        "id": "pW5_A_SmwoSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Batch Size 32 - Accuracy without BN: {accuracy_32}%, with BN: {accuracy_bn_32}%')\n",
        "print(f'Batch Size 64 - Accuracy without BN: {accuracy_64}%, with BN: {accuracy_bn_64}%')\n",
        "print(f'Batch Size 128 - Accuracy without BN: {accuracy_128}%, with BN: {accuracy_bn_128}%')\n"
      ],
      "metadata": {
        "id": "4Pwq2YctuUdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-7uNaT_YuVJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tu3vKniMuVGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}