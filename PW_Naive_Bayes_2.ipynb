{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Probability that an employee is a smoker given that he/she uses the health insurance plan\n",
        "\n",
        "Given:\n",
        "\n",
        "- P(Uses Insurance) = 0.70\n",
        "- P(Smoker/mUsesÂ Insurance)=0.40\n",
        "We need to find P(Smoker/Uses Insurance).\n",
        "\n",
        "This is directly given as 0.40. So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.40 or 40%.\n",
        "\n",
        "Q2. Difference between Bernoulli Naive Bayes and Multinomial Naive Bayes\n",
        "\n",
        "Bernoulli Naive Bayes:\n",
        "- Used for binary/boolean features.\n",
        "- Each feature is modeled as a binary variable (0 or 1).\n",
        "- Suitable for tasks where features represent binary occurrence of attributes (e.g., word presence in text classification).\n",
        "- Assumes that the data follows a Bernoulli distribution.\n",
        "- Takes into account the absence of a feature as well.\n",
        "\n",
        "Multinomial Naive Bayes:\n",
        "- Used for discrete count features.\n",
        "- Each feature is modeled as a count variable (0, 1, 2, ...).\n",
        "- Suitable for tasks where features represent counts or frequencies (e.g., word counts in text classification).\n",
        "- Assumes that the data follows a multinomial distribution.\n",
        "- Ignores the absence of features, focusing on their counts.\n",
        "\n",
        "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "Bernoulli Naive Bayes does not inherently handle missing values directly. If missing values are present, they need to be preprocessed before applying the Bernoulli Naive Bayes classifier. Common strategies for handling missing values include:\n",
        "\n",
        "- Imputation: Replace missing values with a representative value such as the mean, median, or mode.\n",
        "- Removal: Remove records with missing values, though this might lead to loss of valuable data.\n",
        "- Binary Encoding: If the missing values represent a meaningful category, they can be encoded as a separate binary feature.\n",
        "\n",
        "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes assumes that the continuous features are normally distributed. For multi-class classification, the classifier calculates the probability for each class and assigns the class with the highest probability to the instance. The model handles multiple classes by fitting a Gaussian distribution to the data in each class and using these distributions to calculate the probabilities for each class given the features."
      ],
      "metadata": {
        "id": "U_ZLN0WzpCuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Assignment:\n",
        "Data preparation:\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
        "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
        "is spam or not based on several input features.\n",
        "Implementation:\n",
        "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
        "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
        "dataset. You should use the default hyperparameters for each classifier.\n",
        "Results:\n",
        "Report the following performance metrics for each classifier:\n",
        "Accuracy\n",
        "Precision\n",
        "Recall\n",
        "F1 score\n",
        "Discussion:\n",
        "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
        "the case? Are there any limitations of Naive Bayes that you observed?\n",
        "Conclusion:\n",
        "Summarise your findings and provide some suggestions for future work."
      ],
      "metadata": {
        "id": "U2fhlVY4_Jgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "\n",
        "# Load the dataset\n",
        "data_path = '/content/spambase.data'\n",
        "columns = [\n",
        "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n",
        "    \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n",
        "    \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
        "    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
        "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\",\n",
        "    \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\",\n",
        "    \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n",
        "    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
        "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\",\n",
        "    \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_semicolon\", \"char_freq_left_paren\",\n",
        "    \"char_freq_left_bracket\", \"char_freq_exclamation\", \"char_freq_dollar\", \"char_freq_hash\", \"capital_run_length_average\",\n",
        "    \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"\n",
        "]\n",
        "df = pd.read_csv(data_path, header=None, names=columns)\n",
        "\n",
        "# Split data into features and labels\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Initialize 10-fold cross-validation\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize classifiers\n",
        "bnb = BernoulliNB()\n",
        "mnb = MultinomialNB()\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Function to evaluate the classifier\n",
        "def evaluate_classifier(clf, X, y, kf):\n",
        "    accuracy = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')\n",
        "    precision = cross_val_score(clf, X, y, cv=kf, scoring='precision')\n",
        "    recall = cross_val_score(clf, X, y, cv=kf, scoring='recall')\n",
        "    f1 = cross_val_score(clf, X, y, cv=kf, scoring='f1')\n",
        "    return accuracy.mean(), precision.mean(), recall.mean(), f1.mean()\n",
        "\n",
        "# Evaluate classifiers\n",
        "bnb_results = evaluate_classifier(bnb, X, y, kf)\n",
        "mnb_results = evaluate_classifier(mnb, X, y, kf)\n",
        "gnb_results = evaluate_classifier(gnb, X, y, kf)\n",
        "\n",
        "print(\"Bernoulli Naive Bayes: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(*bnb_results))\n",
        "print(\"Multinomial Naive Bayes: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(*mnb_results))\n",
        "print(\"Gaussian Naive Bayes: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(*gnb_results))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7xkfsr--2Mo",
        "outputId": "65de0b3c-ef82-4b67-e10d-7e1a3ef7156c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli Naive Bayes: Accuracy: 0.8857, Precision: 0.8855, Recall: 0.8158, F1 Score: 0.8490\n",
            "Multinomial Naive Bayes: Accuracy: 0.7903, Precision: 0.7407, Recall: 0.7215, F1 Score: 0.7306\n",
            "Gaussian Naive Bayes: Accuracy: 0.8203, Precision: 0.6989, Recall: 0.9575, F1 Score: 0.8078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion of Results\n",
        "1. Bernoulli Naive Bayes:\n",
        "- Accuracy: 0.8857\n",
        "- Precision: 0.8855\n",
        "- Recall: 0.8158\n",
        "- F1 Score: 0.8490\n",
        "\n",
        "The Bernoulli Naive Bayes classifier performed the best overall, especially in terms of accuracy and F1 score. This suggests that this model is particularly well-suited for the binary/boolean nature of the spambase dataset. The high precision indicates that when the model predicts an email as spam, it is usually correct. The recall, while slightly lower, still indicates a good ability to identify most spam emails, balancing well between precision and recall.\n",
        "\n",
        "2. Multinomial Naive Bayes:\n",
        "\n",
        "- Accuracy: 0.7903\n",
        "- Precision: 0.7407\n",
        "- Recall: 0.7215\n",
        "- F1 Score: 0.7306\n",
        "\n",
        "The Multinomial Naive Bayes classifier did not perform as well as the Bernoulli classifier. This model is generally used for features that represent counts or frequencies, which might not align perfectly with the spambase dataset. The relatively lower precision and recall suggest it is less effective at distinguishing between spam and non-spam emails compared to the Bernoulli model.\n",
        "\n",
        "3. Gaussian Naive Bayes:\n",
        "\n",
        "- Accuracy: 0.8203\n",
        "- Precision: 0.6989\n",
        "- Recall: 0.9575\n",
        "- F1 Score: 0.8078\n",
        "\n",
        "The Gaussian Naive Bayes classifier had the highest recall, indicating it is very good at identifying spam emails. However, this came at the cost of lower precision, meaning a higher number of false positives (non-spam emails incorrectly identified as spam). This model assumes features are normally distributed, which may not be the best fit for the spambase dataset.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The Bernoulli Naive Bayes classifier performed the best for the spambase dataset, likely due to its design for binary features, which matches the nature of the dataset. The Multinomial Naive Bayes classifier was less effective, and the Gaussian Naive Bayes classifier, while having high recall, had lower precision.\n",
        "\n",
        "Suggestions for Future Work\n",
        "\n",
        "1. Feature Engineering: Further feature engineering could improve model performance. This might involve removing irrelevant features, adding new derived features, or converting some continuous features into binary features for better compatibility with the Bernoulli model.\n",
        "2. Hyperparameter Tuning: Use GridSearchCV or RandomizedSearchCV to fine-tune the hyperparameters of each Naive Bayes variant to potentially improve performance.\n",
        "3. Ensemble Methods: Consider combining multiple models using ensemble techniques to improve classification performance.\n",
        "4. Explore Other Algorithms: Experiment with other machine learning algorithms such as Support Vector Machines (SVM), Decision Trees, or Random Forests to see if they offer better performance.\n",
        "5. Cross-Validation: Ensure robust evaluation by using cross-validation techniques to validate the models on different subsets of the data."
      ],
      "metadata": {
        "id": "UQwX7-pBB68g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rSfCutCy-5_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}