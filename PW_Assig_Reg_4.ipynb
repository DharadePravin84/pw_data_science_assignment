{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
        "\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes a penalty term to enforce regularization. The penalty term is the L1 norm of the coefficients, which is the sum of the absolute values of the coefficients.\n",
        "\n",
        "Difference from other regression techniques:\n",
        "\n",
        "Ordinary Least Squares (OLS) Regression: Minimizes the sum of squared residuals without any penalty term.\n",
        "\n",
        "Ridge Regression: Adds an L2 norm penalty (sum of squared coefficients) to the loss function, which discourages large coefficients but does not enforce sparsity.\n",
        "\n",
        "Lasso Regression: Adds an L1 norm penalty, which can drive some coefficients to exactly zero, thus performing feature selection.\n",
        "\n",
        "\n",
        "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
        "\n",
        "The main advantage of using Lasso Regression in feature selection is its ability to shrink some coefficients to exactly zero. This property effectively selects a simpler model that includes only the most important features, thus performing automatic feature selection. This can lead to models that are easier to interpret and may generalize better to new data by reducing overfitting.\n",
        "\n",
        "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
        "\n",
        "In Lasso Regression, the coefficients can be interpreted similarly to those in standard linear regression, representing the change in the response variable for a one-unit change in the predictor variable, holding all other predictors constant. However, due to the L1 penalty, some coefficients may be exactly zero, indicating that those features do not contribute to the model.\n",
        "\n",
        "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
        "\n",
        "The primary tuning parameter in Lasso Regression is the regularization parameter λ. It controls the strength of the L1 penalty applied to the coefficients:\n",
        "\n",
        "High λ: Increases the penalty, leading to more coefficients being shrunk to zero, which can result in a simpler model with fewer features.\n",
        "\n",
        "Low λ: Decreases the penalty, making the model closer to standard linear regression with potentially more features included.\n",
        "\n",
        "The choice of λ affects the model's bias-variance trade-off:\n",
        "\n",
        "High λ: Higher bias, lower variance, potentially underfitting.\n",
        "\n",
        "Low λ: Lower bias, higher variance, potentially overfitting.\n",
        "\n",
        "\n",
        "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
        "\n",
        "Lasso Regression is inherently a linear method, but it can be extended to handle non-linear relationships by using techniques such as:\n",
        "\n",
        "Polynomial Features: Transforming the input features to include polynomial terms (e.g., x^2 ,x^3).\n",
        "\n",
        "Basis Functions: Applying non-linear transformations like splines or radial basis functions to the input features before applying Lasso Regression.\n",
        "\n",
        "Kernel Methods: Using kernel tricks to map the input features into a higher-dimensional space where the relationship may be linear.\n",
        "\n",
        "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
        "\n",
        "Ridge Regression and Lasso Regression both add regularization terms to the linear regression loss function, but they use different types of penalties:\n",
        "\n",
        "Ridge Regression: Uses an L2 norm penalty (sum of squared coefficients). It discourages large coefficients but does not set any coefficients to zero, hence it does not perform feature selection.\n",
        "\n",
        "Lasso Regression: Uses an L1 norm penalty (sum of absolute values of coefficients). It can shrink some coefficients to zero, thus performing feature selection.\n",
        "\n",
        "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
        "\n",
        "Yes, Lasso Regression can handle multicollinearity in the input features to some extent. The L1 penalty can shrink some coefficients to zero, which effectively removes redundant features from the model. This reduces the impact of multicollinearity by selecting only one feature from a group of correlated features. However, it might arbitrarily choose one of the correlated features to keep, which may not always be optimal.\n",
        "\n",
        "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
        "\n",
        "To choose the optimal value of the regularization parameter (λ) in Lasso Regression, you can use cross-validation. Here's a concise process:\n",
        "\n",
        "1. Cross-Validation:\n",
        "\n",
        "Split the dataset into k folds (e.g., 5 or 10).\n",
        "\n",
        "For each fold, train the Lasso model on\n",
        "k - 1 folds and validate it on the remaining fold.\n",
        "\n",
        "Compute the average validation error for each λ.\n",
        "\n",
        "Select the λ that minimizes the average validation error."
      ],
      "metadata": {
        "id": "O4BtHeJ8LFpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "# Example dataset creation\n",
        "# Assuming you have a dataset with features and target variable\n",
        "# Replace this with your actual data\n",
        "\n",
        "data = {\n",
        "    'feature1': np.random.randn(100),\n",
        "    'feature2': np.random.randn(100),\n",
        "    'feature3': np.random.randn(100),\n",
        "    'target': np.random.randn(100)\n",
        "}\n",
        "\n",
        "# create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target\n",
        "X = df[['feature1', 'feature2', 'feature3']]\n",
        "y = df['target']\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = 0.2, random_state= 42)\n",
        "\n",
        "# Initialize LassoCV with 5-fold cross-validation\n",
        "lasso_cv = LassoCV(cv=5, random_state=0)\n",
        "\n",
        "# Fit the model to the training data\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "\n",
        "# Optimal lambda\n",
        "optimal_lambda = lasso_cv.alpha_\n",
        "\n",
        "print(f\"The optimal value of lambda is: {optimal_lambda}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMEu5DICOgV2",
        "outputId": "02d991d0-eadc-4fc6-a99b-e23284cff214"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal value of lambda is: 0.0612263583265917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Grid Search\n",
        "\n",
        "Grid search involves specifying a range of λ values and evaluating the model performance for each value. Cross-validation is often used within this process."
      ],
      "metadata": {
        "id": "EBcXEMrSRmSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the model\n",
        "lasso = Lasso()\n",
        "\n",
        "# Define the grid of lambda values\n",
        "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Define the grid search with cross-validation\n",
        "grid_search = GridSearchCV(lasso, param_grid, cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best lambda\n",
        "best_lambda = grid_search.best_params_['alpha']\n",
        "\n",
        "print(f\"The best value of lambda is: {best_lambda}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jef8BK4PA7T",
        "outputId": "b9b525e1-56c6-46a3-88c7-85390fe6085a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best value of lambda is: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cS3H8YPUPGxF"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}