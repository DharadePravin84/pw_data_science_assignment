{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPqp_zm-T-cN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
        "\n",
        "R-squared (R²) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It indicates the goodness of fit of the model."
      ],
      "metadata": {
        "id": "VHQ97RdwUCIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Example data\n",
        "y_true = [3, -0, 2, 7]\n",
        "y_pred = [2.5, 0.0, 2, 8]\n",
        "\n",
        "# Calculate R-squared\n",
        "r_squared = r2_score(y_true, y_pred)\n",
        "print(f\"R-squared: {r_squared}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX4Fca-QVDQ4",
        "outputId": "01e3bd2a-297d-49cc-ba6c-c5e03e31d5b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared: 0.9519230769230769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
        "\n",
        "Adjusted R-squared adjusts the R-squared value based on the number of predictors in the model, providing a more accurate measure when multiple predictors are used.\n",
        "\n",
        "Difference:\n",
        "\n",
        "Regular R-squared can artificially inflate when more predictors are added, even if they don't contribute significantly.\n",
        "Adjusted R-squared penalizes the addition of non-significant predictors, providing a more accurate measure."
      ],
      "metadata": {
        "id": "CgiTBojtVM5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. When is it more appropriate to use adjusted R-squared?\n",
        "\n",
        "Adjusted R-squared is more appropriate when comparing models with a different number of predictors. It helps in determining whether the added predictors improve the model significantly."
      ],
      "metadata": {
        "id": "ePj-mO0DVgSW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2eGYSC6VFw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "- Calculation: The average of the squares of the differences between actual and predicted values.\n",
        "- Representation: Represents the average squared difference between actual and predicted values. Larger errors are penalized more due to squaring.\n",
        "\n",
        "Root Mean Squared Error (RMSE):\n",
        "- Calculation: The square root of MSE.\n",
        "- Representation: Provides the error in the same units as the target variable, making it easier to interpret.\n",
        "\n",
        "Mean Absolute Error (MAE):\n",
        "- Calculation: The average of the absolute differences between actual and predicted values.\n",
        "- Representation: Represents the average absolute difference between actual and predicted values, giving a linear error score."
      ],
      "metadata": {
        "id": "pxXrr7n4lWFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "y_true = np.array([1,2,3,4,5])\n",
        "y_pred = np.array([1.1,2.1,2.9,3.8,5.2])\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "print(f\"MSE: {mse}\")\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "print(f\"MAE: {mae}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hPMBDOxl7Ro",
        "outputId": "20dc378f-ebdf-4364-b345-ead8faf3c813"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.022000000000000037\n",
            "RMSE: 0.1483239697419134\n",
            "MAE: 0.14000000000000012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
        "\n",
        "RMSE:\n",
        "- Advantages: Sensitive to large errors due to squaring; useful when large errors are particularly undesirable.\n",
        "- Disadvantages: More sensitive to outliers than MAE, which may be undesirable in some contexts.\n",
        "\n",
        "MSE:\n",
        "- Advantages: Provides a squared error measure that heavily penalizes large errors, which can be useful for some applications.\n",
        "- Disadvantages: Not in the same units as the target variable, making interpretation less intuitive; sensitive to outliers.\n",
        "\n",
        "MAE:\n",
        "- Advantages: Less sensitive to outliers compared to MSE and RMSE; provides a linear error measure that is easier to interpret.\n",
        "- Disadvantages: May not penalize large errors as strongly as RMSE, which can be a disadvantage if large errors are particularly problematic.\n",
        "\n",
        "\n",
        "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
        "\n",
        "Lasso Regularization (L1):\n",
        "- Concept: Adds a penalty equivalent to the absolute value of the magnitude of coefficients to the loss function.\n",
        "- Effect: Can shrink some coefficients to zero, effectively performing feature selection.\n",
        "\n",
        "Ridge Regularization (L2):\n",
        "- Concept: Adds a penalty equivalent to the square of the magnitude of coefficients to the loss function.\n",
        "- Effect: Shrinks coefficients but does not set any to zero, retaining all features.\n",
        "\n",
        "When to Use:\n",
        "- Lasso: When feature selection is needed, as it can shrink some coefficients to zero.\n",
        "- Ridge: When multicollinearity is present and all features should be retained.\n",
        "\n",
        "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
        "\n",
        "Regularized linear models add a penalty to the loss function to discourage complex models with large coefficients, thus preventing overfitting. This is particularly useful when the number of predictors is large relative to the number of observations."
      ],
      "metadata": {
        "id": "5j1O75BYmvC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Example data\n",
        "X = np.random.rand(100, 10)\n",
        "y = np.dot(X, np.random.rand(10)) + np.random.normal(0, 0.1, 100)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ridge Regression\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge.predict(X_test)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train, y_train)\n",
        "y_pred_lasso = lasso.predict(X_test)\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "\n",
        "print(f\"Ridge MSE: {mse_ridge}, Lasso MSE: {mse_lasso}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9Av3UmtmrVb",
        "outputId": "043cab90-9972-461d-fe09-e441d86bf3e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge MSE: 0.014786441636759143, Lasso MSE: 0.27884989949293965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
        "\n",
        "Limitations:\n",
        "- Bias-Variance Trade-off: Regularization introduces bias to reduce variance, which can lead to underfitting if not balanced properly.\n",
        "- Model Interpretation: The introduction of regularization can make model interpretation more challenging, especially in Lasso where some coefficients are set to zero.\n",
        "- Non-linearity: Regularized linear models are still linear; they might not perform well on data with complex, non-linear relationships.\n",
        "- Choice of Regularization Parameter: Selecting the appropriate regularization parameter (λ) is crucial and can be computationally intensive, typically requiring cross-validation.\n",
        "\n",
        "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
        "\n",
        "Choosing the better performer depends on the context of the problem:\n",
        "- Model B (MAE = 8) would be preferred if the goal is to minimize the average absolute error, especially if outliers are not a major concern.\n",
        "- Model A (RMSE = 10) would be preferred if larger errors are particularly undesirable, as RMSE penalizes larger errors more heavily.\n",
        "\n",
        "Limitations:\n",
        "- RMSE: More sensitive to outliers, which might skew the performance metric if there are extreme values.\n",
        "- MAE: Does not heavily penalize larger errors, which could be an issue if those errors are critical in the specific application.\n",
        "\n",
        "\n",
        "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
        "\n",
        "Choosing the better performer depends on the dataset and the problem context:\n",
        "- Model A (Ridge) would be preferred if all features are considered important and multicollinearity is present.\n",
        "- Model B (Lasso) would be preferred if feature selection is needed, as it can shrink some coefficients to zero.\n",
        "\n",
        "Trade-offs and Limitations:\n",
        "- Ridge: Retains all features but does not perform feature selection.\n",
        "- Lasso: Performs feature selection but might exclude important features if λ is too high.\n",
        "- Regularization Parameter: The choice of λ is critical and can significantly affect model performance; different values of λ should be tested using cross-validation."
      ],
      "metadata": {
        "id": "sS0pKDErn46v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMY7Ip09n2It"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}