{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
        "\n",
        "Ridge Regression:\n",
        "- Definition: Ridge Regression is a type of linear regression that includes a regularization term to prevent overfitting by shrinking the regression coefficients. It is also known as L2 regularization.\n",
        "\n",
        "Differences from Ordinary Least Squares (OLS) Regression:\n",
        "- OLS Regression: Minimizes the sum of squared residuals.\n",
        "- Ridge Regression: Minimizes the sum of squared residuals plus a penalty proportional to the sum of the squares of the coefficients. This additional term helps to prevent overfitting by shrinking the coefficients.\n",
        "\n",
        "Q2. What are the assumptions of Ridge Regression?\n",
        "\n",
        "The assumptions of Ridge Regression are similar to those of OLS regression:\n",
        "\n",
        "1. Linearity: The relationship between the predictors and the response variable is linear.\n",
        "2. Independence: The observations are independent of each other.\n",
        "3. Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables.\n",
        "4. Normality: The residuals (errors) of the model are normally distributed.\n",
        "5. Multicollinearity: Ridge Regression assumes multicollinearity among the predictor variables, which it addresses by shrinking the coefficients.\n",
        "\n",
        "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
        "\n",
        "The value of the tuning parameter λ in Ridge Regression can be selected using\n",
        "cross-validation:\n",
        "\n",
        "- Cross-Validation: Split the data into training and validation sets multiple times, train the model on the training sets, and evaluate it on the validation sets for different values of λ. The value of λ that minimizes the cross-validated error is chosen."
      ],
      "metadata": {
        "id": "6Z4poxkcpOhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generating example data\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 10)\n",
        "y = np.dot(X, np.random.rand(10)) + np.random.normal(0, 0.1, 100)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a range of lambda values\n",
        "alphas = [0.1, 1.0, 10.0]\n",
        "\n",
        "# Use RidgeCV to find the optimal lambda\n",
        "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "\n",
        "# Optimal lambda\n",
        "optimal_lambda = ridge_cv.alpha_\n",
        "print(f\"Optimal lambda: {optimal_lambda}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2-7Wwo3rMlt",
        "outputId": "2a5ed598-26da-4b7b-db69-885f229059f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal lambda: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
        "\n",
        "Ridge Regression is not typically used for feature selection because it does not set coefficients to zero. Instead, it shrinks the coefficients of less important features towards zero but still keeps them in the model. However, this shrinkage can indicate the relative importance of features.\n",
        "\n",
        "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
        "\n",
        "Ridge Regression performs well in the presence of multicollinearity. Multicollinearity occurs when predictor variables are highly correlated, leading to unstable coefficient estimates in OLS regression. Ridge Regression addresses this by adding a penalty term that shrinks the coefficients, thereby reducing variance and improving model stability.\n",
        "\n",
        "Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
        "\n",
        "Yes, Ridge Regression can handle both categorical and continuous independent variables. However, categorical variables need to be encoded appropriately (e.g., using one-hot encoding) before being used in the regression model.\n",
        "\n",
        "Q7. How do you interpret the coefficients of Ridge Regression?\n",
        "\n",
        "The coefficients in Ridge Regression are interpreted similarly to those in OLS regression, with the understanding that they are shrunk towards zero due to the regularization term. The magnitude of the coefficients is typically smaller, reflecting the regularization's effect in reducing overfitting and multicollinearity issues.\n",
        "\n",
        "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
        "\n",
        "Yes, Ridge Regression can be used for time-series data analysis. The process involves:\n",
        "\n",
        "1. Lagged Variables: Creating lagged versions of the time-series data as predictor variables.\n",
        "2. Regularization: Applying Ridge Regression to account for potential multicollinearity among the lagged variables."
      ],
      "metadata": {
        "id": "ahYYONYMrSxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example time-series data\n",
        "data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "df = pd.concat([data.shift(i) for i in range(3)], axis=1)\n",
        "df.columns = ['lag_2', 'lag_1', 'current']\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Split data\n",
        "X = df[['lag_2', 'lag_1']]\n",
        "y = df['current']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Ridge Regression\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "y_pred = ridge.predict(X_test)\n"
      ],
      "metadata": {
        "id": "DsOxXN8urQhi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-UEp1gYrkS-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}