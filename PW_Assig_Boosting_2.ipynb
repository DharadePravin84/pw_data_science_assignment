{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q1. What is Gradient Boosting Regression?\n",
        "\n",
        "Gradient Boosting Regression is an ensemble learning technique that combines multiple weak predictive models, typically decision trees, to create a strong predictive model. In the context of regression, the goal is to predict continuous values by minimizing a loss function, often the mean squared error. The algorithm iteratively adds trees to the model, each one correcting the errors of the previous trees by fitting to the negative gradient of the loss function."
      ],
      "metadata": {
        "id": "6nlfZbq8u_gC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
      ],
      "metadata": {
        "id": "SFT6cUsQvHlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([3, 6, 9, 12, 15, 18, 21, 24, 27, 30])\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize predictions\n",
        "        y_pred = np.zeros_like(y, dtype=float)\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Calculate residuals\n",
        "            residuals = y - y_pred\n",
        "            # Train a weak learner on residuals\n",
        "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            model.fit(X, residuals)\n",
        "            # Update predictions\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0], dtype=float)\n",
        "        for model in self.models:\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Train the model\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "gbr.fit(X, y)\n",
        "y_pred = gbr.predict(X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-Squared: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwr-kaKzvDgz",
        "outputId": "4cc6b9bf-7b09-4da8-cd5e-8385ea43fc56"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 2.594340990572291e-07\n",
            "R-Squared: 0.9999999965059381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model. Use grid search or random search to find the best hyperparameters."
      ],
      "metadata": {
        "id": "wDSJlbSlvWtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([3, 6, 9, 12, 15, 18, 21, 24, 27, 30])\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.models = []  # Reset models list\n",
        "        y_pred = np.zeros_like(y, dtype=float)\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            model.fit(X, residuals)\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0], dtype=float)\n",
        "        for model in self.models:\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {'n_estimators': self.n_estimators, 'learning_rate': self.learning_rate, 'max_depth': self.max_depth}\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for key, value in params.items():\n",
        "            setattr(self, key, value)\n",
        "        return self\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Grid search for hyperparameter optimization\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [1, 3, 5]\n",
        "}\n",
        "\n",
        "gbr = GradientBoostingRegressor()\n",
        "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "\n",
        "# Train final model with best parameters\n",
        "best_params = grid_search.best_params_\n",
        "gbr = GradientBoostingRegressor(**best_params)\n",
        "gbr.fit(X_train, y_train)\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-Squared: {r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14r-FNPGvTbz",
        "outputId": "4cfd7b12-c156-46ac-e73a-8781275e173b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 200}\n",
            "Mean Squared Error: 8.997495876851984\n",
            "R-Squared: 0.9183900600739049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is a weak learner in Gradient Boosting?\n",
        "\n",
        "A weak learner in Gradient Boosting is a model that performs slightly better than random guessing. In the context of decision trees, a weak learner is often a shallow tree (a tree with few splits). The idea is that by combining many weak learners, each one making small improvements, the overall model will be much stronger."
      ],
      "metadata": {
        "id": "31TEOV_cwqSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "\n",
        "The intuition behind Gradient Boosting is to build a strong predictive model by combining multiple weak models in a sequential manner. Each new model is trained to correct the errors made by the previous models. This is done by fitting each new model to the negative gradient of the loss function with respect to the predictions of the ensemble. By iteratively reducing the residual errors, the ensemble improves its predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "uCViW1VKwuaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "\n",
        "Gradient Boosting builds an ensemble of weak learners as follows:\n",
        "\n",
        "1. Initialize the model with a constant prediction (usually the mean of the target variable).\n",
        "2. Compute the residuals (errors) between the true target values and the current model predictions.\n",
        "3. Fit a weak learner (e.g., a decision tree) to these residuals.\n",
        "4. Update the model by adding the predictions of the weak learner, scaled by a learning rate.\n",
        "5. Repeat steps 2-4 for a specified number of iterations or until the residuals are minimized."
      ],
      "metadata": {
        "id": "ZIeockl5w1Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
      ],
      "metadata": {
        "id": "7getEM5UxT_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([3, 6, 9, 12, 15, 18, 21, 24, 27, 30])\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_pred = np.zeros_like(y, dtype=float)\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            model.fit(X, residuals)\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0], dtype=float)\n",
        "        for model in self.models:\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Grid search for hyperparameter optimization\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [1, 3, 5]\n",
        "}\n",
        "\n",
        "best_params = None\n",
        "best_score = float('inf')\n",
        "\n",
        "for n_estimators in param_grid['n_estimators']:\n",
        "    for learning_rate in param_grid['learning_rate']:\n",
        "        for max_depth in param_grid['max_depth']:\n",
        "            gbr = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
        "            gbr.fit(X_train, y_train)\n",
        "            y_pred = gbr.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            if mse < best_score:\n",
        "                best_score = mse\n",
        "                best_params = {'n_estimators': n_estimators, 'learning_rate': learning_rate, 'max_depth': max_depth}\n",
        "\n",
        "print(f'Best Parameters: {best_params}')\n",
        "\n",
        "# Train final model with best parameters\n",
        "gbr = GradientBoostingRegressor(**best_params)\n",
        "gbr.fit(X_train, y_train)\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-Squared: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOqVs1vqwryP",
        "outputId": "de2bf6fe-dd6a-4665-ef81-b6cb4a8f18b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 1}\n",
            "Mean Squared Error: 8.08655237415213\n",
            "R-Squared: 0.9266525861754908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXv_xOdnxWVS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}