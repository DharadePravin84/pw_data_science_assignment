{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77042a3-14c1-4a11-8ac4-d20ad67b3f96",
   "metadata": {},
   "source": [
    "#### Q1. What is the KNN algorithm?\n",
    "Ans:\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, instance-based learning algorithm used for both classification and regression tasks. It makes predictions based on the majority class or average value of the K nearest data points in the feature space.\n",
    "\n",
    "#### Q2. How do you choose the value of K in KNN?\n",
    "Ans: \n",
    "\n",
    "The value of K in KNN is typically chosen using techniques like cross-validation or by testing different values and selecting the one that gives the best performance on validation data. Choosing a smaller K value can result in a more flexible model, prone to noise, while a larger K value can lead to a smoother decision boundary but might overlook local patterns.\n",
    "\n",
    "#### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "Ans: \n",
    "\n",
    "KNN classifier predicts the class of a new data point based on the majority class of its K nearest neighbors, while KNN regressor predicts the continuous value of a new data point based on the average value of its K nearest neighbors.\n",
    "\n",
    "#### Q4. How do you measure the performance of KNN?\n",
    "Ans:\n",
    "\n",
    "The performance of KNN can be measured using evaluation metrics such as accuracy, precision, recall, F1-score for classification tasks, and metrics like mean squared error (MSE), R-squared for regression tasks.\n",
    "\n",
    "#### Q5. What is the curse of dimensionality in KNN?\n",
    "Aa\n",
    "The curse of dimensionality refers to the phenomenon where the feature space becomes increasingly sparse as the number of dimensions (features) increases. This can lead to increased computational complexity and decreased performance of KNN due to difficulties in defining meaningful distances between data points.\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Missing values in KNN can be handled by imputing them with the mean, median, or mode of the respective feature. Alternatively, techniques like KNN imputation can be used to estimate missing values based on the values of neighboring data points.\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "KNN classifier is suitable for classification tasks where the output is categorical, while KNN regressor is suitable for regression tasks where the output is continuous. The choice between them depends on the nature of the problem and the type of output variable being predicted.\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "Strengths: KNN is simple to understand and implement, robust to noisy data, and can capture complex patterns in the data.\n",
    "Weaknesses: KNN can be computationally expensive for large datasets, sensitive to the choice of distance metric and K value, and requires a significant amount of memory to store all training data. Techniques like dimensionality reduction, distance metric optimization, and model ensembling can address some of these weaknesses.\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in Euclidean space, while Manhattan distance is the sum of the absolute differences between the coordinates of two points. Euclidean distance is sensitive to the magnitude of differences in all dimensions, while Manhattan distance is sensitive to the sum of absolute differences in each dimension.\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling is important in KNN because it helps to ensure that all features contribute equally to the distance calculations. Without scaling, features with larger scales can dominate the distance calculations, leading to biased results. Common techniques for feature scaling include normalization and standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756567ed-a737-4c18-92ad-4a512a6ae207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
