{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92ed5ac-5ab8-458c-9402-fcd7f8fff9f2",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "\n",
    "- Ans : \n",
    "Missing values in a dataset are absent or undefined data points. It's essential to handle them because they can bias analysis, reduce statistical power, and impact model accuracy. Some algorithms unaffected by missing values include:\n",
    "\n",
    "1. Tree-based models (e.g., Decision Trees, Random Forests).\n",
    "2. k-Nearest Neighbors (k-NN).\n",
    "3. Naive Bayes.\n",
    "4. Support Vector Machines (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e17ad-94ae-4da1-866d-797273e17a41",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c10d1-70b2-486b-b825-83a2e8275d21",
   "metadata": {},
   "source": [
    "1. Deletion of missing data:\n",
    "- Drop rows or columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c25041-e315-4773-9e24-a3487acb1a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# example:\n",
    "df = pd.DataFrame({'A' : [1, 2, None, 4],\n",
    "                   'B' : [5, None , 7, 8]}\n",
    "                 )\n",
    "# drop rows with any missing values \n",
    "df_drop_rows = df.dropna()\n",
    "print(df_drop_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3652686-cc12-4006-a66d-a379da0dd1b4",
   "metadata": {},
   "source": [
    "2. Inputation with mean, meadian or mode :\n",
    "- Fill missing values with the mean , meadian, mode of the respective column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf66da40-b6af-4678-a754-828fbe05cf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "# Input missing values with mean\n",
    "df_mean_imputed = df.fillna(df.mean())\n",
    "print(df_mean_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d6ad2-3e63-49bc-a78f-2c4e68be3261",
   "metadata": {},
   "source": [
    "3. Forward Fill (or Backward fill):\n",
    "- Fill missing values with the previous(or next) non-missing value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5b53d4-4101-4322-90a8-dbbf649df10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  5.0\n",
      "2  2.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "## forward fill missing values \n",
    "df_ffill = df.ffill()\n",
    "print(df_ffill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b06b87f-1783-4b28-a574-1694a8469d7a",
   "metadata": {},
   "source": [
    "4. Interpolation :\n",
    "- Use interpolation methods to estimate missing values based on existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6031797b-2380-4eb6-828b-9a3c1d260402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# Linear interpolation\n",
    "df_interpolatd  = df.interpolate()\n",
    "print(df_interpolatd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9fe34-de94-4ac4-922c-0cfb2e62de43",
   "metadata": {},
   "source": [
    "3. Model- Based Imputation:\n",
    "- Use machine learning models to predict missing values based on other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "736adf73-7f3e-45c2-b211-fea5eda3b9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0  5.00\n",
      "1  2.0  6.54\n",
      "2  2.5  7.00\n",
      "3  4.0  8.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Impute missing values using RandomForestRegressor\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=0)\n",
    "df_model_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_model_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2435d1-f358-4888-920d-f64adef35855",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled\n",
    "- Ans: \n",
    "    Imbalanced data refers to a situation in a classification problem where the distribution of classes is not equal or nearly equal. In other words, one class has significantly fewer instances than another class. This imbalance can have notable consequences for the performance of machine learning models.\n",
    "    \n",
    "1. Bias and Misclassification: Models tend to favor the majority class, leading to biased predictions and misclassification of the minority class.\n",
    "\n",
    "2. Loss of Information: Features and patterns associated with the minority class may not be effectively learned, resulting in a loss of valuable information.\n",
    "\n",
    "3. Incorrect Evaluation: Traditional metrics like accuracy can be misleading; more focused metrics like precision, recall, or AUC-ROC should be considered.\n",
    "\n",
    "- Handling techniques:\n",
    "\n",
    "1. Resampling: Undersample the majority or oversample the minority class.\n",
    "2. Synthetic Data: Use techniques like SMOTE to generate synthetic samples.\n",
    "3. Algorithm Selection: Choose models less sensitive to imbalance, like Random Forests or tuned XGBoost.\n",
    "4. Cost-Sensitive Learning: Adjust algorithms to account for misclassification costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36616bf2-b9db-4777-9990-55faf18cafc5",
   "metadata": {},
   "source": [
    "Q4.  What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required.\n",
    "- Ans: Up-sampling and down-sampling are techniques used to address class imbalance in a dataset:\n",
    "\n",
    "1. Up-sampling (Over-sampling):\n",
    "\n",
    "-Definition: Up-sampling involves increasing the number of instances in the minority class by either replicating existing instances or generating synthetic samples.\n",
    "\n",
    "-Example Scenario: Suppose you have a dataset with 90% of instances belonging to the majority class (Class A) and only 10% belonging to the minority class (Class B). In this case, you might up-sample Class B to achieve a more balanced distribution, making the training set contain equal or nearly equal instances of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2dc279d-cd4f-495b-9be4-b91ef2efc7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'feature': [1, 2, 3, 4, 5, 6, 7, 8, 9 , 10],\n",
    "    'class' : ['A', 'A', 'A', 'A','A', 'B', 'B', 'B' ,'B', 'B']\n",
    "})\n",
    "\n",
    "# Saperate majority and minority classes\n",
    "majority_class = df[df['class'] == 'A']\n",
    "minority_class = df[df['class'] == 'B']\n",
    "\n",
    "#UP- sample minority class\n",
    "minority_upsampled = resample(minority_class, replace = True, n_samples=len(majority_class), random_state=42)\n",
    "\n",
    "# Combine majority class with up-sampled minority Class\n",
    "df_upsampled = pd.concat([majority_class, minority_upsampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895cc131-85d2-4935-b306-79c3b2a164fb",
   "metadata": {},
   "source": [
    "2. Down-sampling(Under-sampling):\n",
    "- Definition : Dowm-Sampling involves reducing the number of instances in the majority class by reandomly removing instances or using other methods.\n",
    "- Example Scenario: In a dataset where class A has 90% of instances and class B has 10% , you might down-sample calss A to create a more balanced distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d077ed7-01af-4c3f-ade7-21219e616b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down-sample majority class\n",
    "majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "\n",
    "\n",
    "# combine down-sampled majority class with minority class\n",
    "df_downsampled = pd.concat([majority_downsampled, minority_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a32e73-4b60-4a14-bcbe-bdf3924580f0",
   "metadata": {},
   "source": [
    "When to use up-sampling and down-sampling:\n",
    "\n",
    "Up-sampling:\n",
    "\n",
    "When the minority class is under-represented and needs more instances for the model to learn effectively.\n",
    "When there is a risk of the model being biased towards the majority class.\n",
    "Down-sampling:\n",
    "\n",
    "When the majority class is significantly larger, and reducing its size improves the balance.\n",
    "When computational resources are limited, and a smaller dataset is preferred.\n",
    "The choice between up-sampling and down-sampling depends on the specific characteristics of the dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b8db8-0b56-464a-b7a0-a037e143ace1",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "- Ans:\n",
    "\n",
    "- Data Augmentation:\n",
    "Data augmentation is a technique used to artificially increase the diversity of a dataset by applying various transformations to the existing data, creating new instances. This is commonly used in computer vision tasks, such as image classification, to enhance model generalization and robustness.\n",
    "\n",
    "- For example, in image data augmentation, one might create new training examples by applying random rotations, flips, zooms, or shifts to the original images. This results in a more varied dataset for training, preventing the model from overfitting to the specific examples in the training set.\n",
    "\n",
    "- SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "SMOTE is a specific technique used for addressing class imbalance in machine learning datasets, particularly in the context of classification problems where one class is under-represented. Rather than simply duplicating minority class instances (as in up-sampling), SMOTE generates synthetic samples to balance the class distribution.\n",
    "\n",
    "- Here's how SMOTE works:\n",
    "1. Select a Minority Instance: For each minority instance, SMOTE selects a particular instance from the minority class.\n",
    "\n",
    "2. Find k Nearest Neighbors: SMOTE identifies the k nearest neighbors of the selected instance within the minority class.\n",
    "\n",
    "3. Generate Synthetic Instances: Synthetic instances are created by interpolating between the selected instance and its k nearest neighbors. This is done by selecting a random value between 0 and 1 for each feature and applying it to the difference between the feature values of the nearest neighbor and the selected instance.\n",
    "\n",
    "4. Repeat: Steps 1-3 are repeated until the desired balance between the minority and majority classes is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cc35d60-7b5f-4628-b633-aee7ba7eb653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6af51-5740-4998-a47b-d8daf628dffd",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "- Ans: Outliers in a Dataset:\n",
    "Outliers are data points that significantly deviate from the majority of the observations in a dataset. They can be unusually high or low values that don't follow the general pattern of the data.\n",
    "\n",
    "- Importance of Handling Outliers:\n",
    "Handling outliers is essential for several reasons:\n",
    "\n",
    "1. Impact on Statistical Measures: Outliers can distort statistical measures like mean and standard deviation, leading to inaccurate insights about the central tendency and variability of the data.\n",
    "\n",
    "2. Influence on Models: Outliers can have a disproportionate impact on model training, especially for algorithms sensitive to extreme values. This can result in biased model parameters and poor generalization to new data.\n",
    "\n",
    "3. Assumption Violation: Many statistical and machine learning models assume that the data follows a certain distribution. Outliers can violate these assumptions, affecting the validity of the models.\n",
    "\n",
    "4. Model Robustness: Outliers can introduce noise and reduce the robustness of predictive models, making them less reliable in real-world scenarios.\n",
    "\n",
    "5. Data Visualization: Outliers can distort the visualization of data patterns, making it challenging to interpret and communicate findings effectively.\n",
    "\n",
    "6. Risk of Misinterpretation: Ignoring outliers may lead to misinterpretation of trends and patterns in the data, affecting the quality of analytical insights.\n",
    "\n",
    "- Common Techniques for Handling Outliers:\n",
    "\n",
    "1. Truncation or Capping: Limit extreme values to a specified threshold.\n",
    "2. Transformation: Apply mathematical transformations like logarithm or square root to reduce the impact of outliers.\n",
    "3. Imputation: Replace outliers with more typical values based on statistical measures.\n",
    "4. Winsorizing: Replace extreme values with values closer to the mean or median.\n",
    "5. Removing Outliers: Delete or separate extreme values from the dataset (use caution, as it can lead to data loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59a104-9aa1-40b6-b8ab-fd2567d6688d",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of  the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "- Ans:\n",
    "- Handling Missing Data in Customer Analysis:\n",
    "\n",
    "1. Data Imputation:\n",
    "\n",
    "- Mean, median, or mode imputation.\n",
    "- Forward fill, backward fill, or interpolation.\n",
    "- Model-based imputation, KNN imputation.\n",
    "\n",
    "2.Deletion:\n",
    "\n",
    "- Listwise deletion for random missing data.\n",
    "- Column deletion for less crucial columns.\n",
    "\n",
    "3. Advanced Techniques:\n",
    "\n",
    "- Multiple imputation for robustness.\n",
    "- Domain-specific imputation based on knowledge.\n",
    "- Imputation using external datasets.\n",
    "\n",
    "4. Flagging Missing Data:\n",
    "\n",
    "- Use an indicator variable to identify missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c7204-1531-439f-a5ae-ae54aac4160a",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "- Ans:\n",
    "To determine if missing data is missing at random or if there is a pattern, you can employ various strategies:\n",
    "\n",
    "1. Descriptive Statistics:\n",
    "- Calculate summary statistics for the variables with missing data.\n",
    "- Compare these statistics between the cases with missing values and those without.\n",
    "\n",
    "2. Visualization:\n",
    "- Use data visualization techniques, such as histograms, box plots, or heatmaps, to visually inspect patterns in missing data.\n",
    "- Plot the missingness pattern across variables or over time.\n",
    "\n",
    "3. Correlation Analysis: \n",
    "- Examine the correlation between the missingness of one variable and the missingness of other variables.\n",
    "- Compute correlation coefficients or use graphical tools like heatmaps.\n",
    "\n",
    "4. Missingness Tests:\n",
    "- Apply statistical tests to assess if the missingness is significantly associated with certain variables.\n",
    "- Chi-square tests or logistic regression can be used for categorical variables.\n",
    "\n",
    "5. Time Series Analysis:\n",
    "- If the data has a temporal component, analyze if missingness follows a specific trend over time.\n",
    "- Plot the missingness pattern against time to identify any temporal patterns.\n",
    "\n",
    "6. Domain Knowledge:\n",
    "- Leverage domain knowledge to understand if certain conditions or events might be associated with missing data.\n",
    "- Consult with subject matter experts to gain insights into potential patterns.\n",
    "\n",
    "7. Missing Data Indicators:\n",
    "- Create indicator variables that flag whether data is missing or not.\n",
    "- Assess relationships between these indicators and other variables.\n",
    "\n",
    "8. Imputation Impact:\n",
    "- Assess the impact of different imputation strategies on the results.\n",
    "- If imputations significantly alter the conclusions, there might be a non-random pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79191478-e018-4d33-a9f7-49b30d635d1f",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "- Ans: \n",
    "\n",
    "Dealing with imbalanced datasets in a medical diagnosis project requires thoughtful strategies for evaluating model performance. Here are some approaches:\n",
    "\n",
    "1. Confusion Matrix Analysis:\n",
    "\n",
    "- Examine the confusion matrix to understand true positives, true negatives, false positives, and false negatives.\n",
    "- Metrics such as precision, recall, F1-score, and specificity are more informative than accuracy in imbalanced scenarios.\n",
    "\n",
    "2. Precision and Recall:\n",
    "\n",
    "- Precision (Positive Predictive Value) measures the accuracy of positive predictions.\n",
    "- Recall (Sensitivity or True Positive Rate) assesses the ability to capture all positive instances.\n",
    "- A balance between precision and recall is crucial for an effective model.\n",
    "\n",
    "3. F1-Score:\n",
    "\n",
    "- The F1-score is the harmonic mean of precision and recall, providing a single metric for model evaluation.\n",
    "- It is especially useful when there is an uneven class distribution.\n",
    "\n",
    "4. Area Under the Receiver Operating Characteristic (ROC-AUC) Curve:\n",
    "\n",
    "- ROC-AUC considers the trade-off between true positive rate and false positive rate across different probability thresholds.\n",
    "- Particularly useful for binary classification problems.\n",
    "\n",
    "5. Cost-Sensitive Learning:\n",
    "\n",
    "- Adjust the model's objective function to account for misclassification costs, assigning higher penalties for misclassifying the minority class.\n",
    "\n",
    "6. Resampling Techniques:\n",
    "\n",
    "- Implement up-sampling (creating synthetic samples for the minority class) or down-sampling (reducing the majority class instances) to balance the dataset.\n",
    "- Evaluate the model on the balanced dataset.\n",
    "\n",
    "7. Ensemble Methods:\n",
    "\n",
    "- Utilize ensemble models like Random Forests or Gradient Boosting, which are inherently more robust to imbalanced datasets.\n",
    "\n",
    "8. Threshold Adjustment:\n",
    "\n",
    "- Experiment with adjusting the classification threshold to balance precision and recall based on the specific requirements of the medical diagnosis task.\n",
    "\n",
    "9. Stratified Cross-Validation:\n",
    "\n",
    "- Use stratified cross-validation to ensure that each fold maintains the same class distribution as the entire dataset.\n",
    "\n",
    "10. Domain Expert Consultation:\n",
    "\n",
    "- Consult with medical experts to gain insights into the criticality of false positives and false negatives for the specific medical condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51797ce2-a8d6-42b5-ac66-c08702e41ca2",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "\n",
    "- Ans :\n",
    "Balancing Customer Satisfaction Dataset:\n",
    "\n",
    "1. Random Under-Sampling:\n",
    "- Randomly remove instances from the majority class.\n",
    "\n",
    "2. Cluster-Centroids Under-Sampling:\n",
    "- Use ClusterCentroids to generate centroids based on clustering.\n",
    "\n",
    "3. Tomek Links:\n",
    "- Identify and remove majority class instances involved in Tomek links.\n",
    "\n",
    "4. NearMiss Under-Sampling:\n",
    "- Use NearMiss to select majority class instances close to the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe6526-8387-4e56-b181-21116858c6b2",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "\n",
    "- Ans: \n",
    "When dealing with an imbalanced dataset, especially when estimating a rare event with a low percentage of occurrences, you can use up-sampling techniques to increase the number of instances in the minority class. Here are some methods:\n",
    "\n",
    "1.Random Over-Sampling:\n",
    "- Randomly replicate instances from the minority class.\n",
    "\n",
    "2. SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "- Generate synthetic instances for the minority class.\n",
    "\n",
    "3. ADASYN (Adaptive Synthetic Sampling):\n",
    "- Adapt synthetic sample generation based on local density of the minority class.\n",
    "\n",
    "4. Random Over-Sampling with Replacement:\n",
    "- Randomly replicate instances from the minority class with replacement.\n",
    "\n",
    "5. Ensemble Techniques:\n",
    "- Use ensemble methods like Random Forests for inherent robustness.\n",
    "\n",
    "6. Weighted Models:\n",
    "- Assign higher weights to the minority class during model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
