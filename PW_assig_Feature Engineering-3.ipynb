{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48cccf2-7429-4b27-a305-e328e2a2130a",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "- Ans:\n",
    "- Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features to a specific range. The purpose of Min-Max scaling is to ensure that all features have the same scale, preventing certain features from dominating the learning process just because they have larger magnitudes.\n",
    "- Here's an example in python to illustrate Min-Max scaling using the popular scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3985f278-7abb-404c-8dfc-3ab637c25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Scaled Data\n",
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data =  np.array([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "\n",
    "# Instantiate MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data\")\n",
    "print(data)\n",
    "print(\"\\nScaled Data\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcb95e-0df2-4129-99bc-6b2936db277c",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "- Ans:\n",
    "- The Unit Vector technique, also known as vector normalization, is a feature scaling method that scales individual samples (rows) in a dataset to have a length of 1. This technique is particularly useful when the magnitude or scale of individual samples is important, and you want to emphasize the direction of the data points rather than their absolute magnitudes.\n",
    "- Here's an example in Python to illustrate the Unit Vector scaling using the NumPy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346290fd-4dbb-4da1-b3ab-9381c3bf56d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Unit Vector Scaled Data:\n",
      "[[0.26726124 0.53452248 0.80178373]\n",
      " [0.45584231 0.56980288 0.68376346]\n",
      " [0.50257071 0.57436653 0.64616234]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# sample data \n",
    "# calculate the Euclidean norm for each row\n",
    "norms = np.linalg.norm(data, axis=1 , keepdims=True)\n",
    "\n",
    "# Perform Unit Vector scaling\n",
    "unit_vector_scaled_data = data / norms\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nUnit Vector Scaled Data:\")\n",
    "print(unit_vector_scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ff875-0fbc-49e5-b8d4-2816fa4204b7",
   "metadata": {},
   "source": [
    "The key difference between Unit Vector scaling and Min-Max scaling is that Unit Vector scaling focuses on normalizing the individual samples (rows) rather than the features (columns). It's particularly relevant when you want to compare or analyze data points based on their directions in a high-dimensional space.\n",
    "\n",
    "While Min-Max scaling ensures that features have the same scale across the entire dataset, Unit Vector scaling emphasizes the direction of each data point, making it suitable for scenarios where the magnitude of individual samples is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd09186-3849-4ff0-91db-1efaf4c6b74e",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "- Ans:\n",
    "\n",
    "- Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis. The main goal of PCA is to transform a dataset with possibly correlated features into a new set of uncorrelated features, called principal components. These principal components are ordered in terms of decreasing variance, so the first few components retain most of the variability present in the original data.\n",
    "\n",
    "- The steps involved in PCA are as follows:\n",
    "\n",
    "1. Standardization: Standardize the dataset to have zero mean and unit variance. This step is important because PCA is sensitive to the scale of the features.\n",
    "\n",
    "2. Calculate Covariance Matrix: Compute the covariance matrix of the standardized data.\n",
    "\n",
    "3. Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix. This results in eigenvalues and eigenvectors.\n",
    "\n",
    "4. Sort Eigenvalues: Sort the eigenvalues in descending order and correspondingly arrange the eigenvectors.\n",
    "\n",
    "5. Select Principal Components: Choose the top 'k' eigenvectors based on the explained variance or a predetermined number of components.\n",
    "\n",
    "6. Projection: Project the original data onto the selected principal components to obtain the reduced-dimensional representation.\n",
    "\n",
    "- Here's an example using Python with the popular scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1aaaf4-dde1-449e-ae8b-f08582889817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape :  (150, 4)\n",
      "Reduced Data shape:  (150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# satndardize the data :\n",
    "X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Apply PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Data Shape : \", X_standardized.shape)\n",
    "print(\"Reduced Data shape: \", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aed247-05e6-4ce4-8881-d71585000ea1",
   "metadata": {},
   "source": [
    "In this example, the iris dataset is loaded, and the features are standardized. PCA is then applied with the argument n_components=2, meaning we want to reduce the data to two dimensions. The transformed data (X_pca) now has only two columns, representing the two principal components.\n",
    "\n",
    "PCA is useful for reducing the dimensionality of a dataset while retaining most of its variability. It is commonly employed for visualization, noise reduction, and improving the efficiency of machine learning algorithms by working with a smaller set of informative features. The choice of the number of principal components depends on the desired level of explained variance and the specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30eda0-facb-44c8-b9b3-95e46387d064",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "- Ans:\n",
    "\n",
    "PCA (Principal Component Analysis) can be used for feature extraction, and it is essentially a technique that combines both dimensionality reduction and feature extraction. In the context of PCA, feature extraction refers to transforming the original features into a new set of features (principal components) that captures the most important information in the data.\n",
    "\n",
    "The relationship between PCA and feature extraction can be summarized as follows:\n",
    "\n",
    "Reduction of Dimensionality: PCA reduces the dimensionality of the data by transforming it into a new space defined by the principal components. These components are linear combinations of the original features.\n",
    "\n",
    "Feature Ranking by Importance: Principal components are ordered by their associated eigenvalues, and the first few principal components capture the majority of the variance in the data. As a result, the most important features are implicitly ranked in terms of their contribution to the variance.\n",
    "\n",
    "New Feature Representation: The principal components themselves serve as new features that are uncorrelated and capture the maximum variance in the dataset. These can be used as a reduced set of features that retains essential information.\n",
    "\n",
    "Here's an example using scikit-learn in Python to demonstrate how PCA can be used for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d2ba07-ec99-41e7-940f-01947124d777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape:  (150, 4)\n",
      "Reduced Data Shape (after PCA) :  (150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Apply PCA with 2 components for feature extraction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Data Shape: \", X.shape)\n",
    "print(\"Reduced Data Shape (after PCA) : \" , X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02271058-86f8-4463-af2b-c0e011304af9",
   "metadata": {},
   "source": [
    "\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "- Ans : \n",
    "\n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling is employed to preprocess data features such as price, rating, and delivery time. This technique transforms numerical features to a common scale, typically [0, 1], ensuring uniform influence across different features. The Min-Max scaling formula is applied to each feature, using a scaler like scikit-learn's MinMaxScaler. This normalization facilitates fair contribution of each feature to the recommendation model, preventing dominance by features with larger magnitudes. Consistency in applying the scaling transformation is crucial for both training and deploying the recommendation system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728c051-3512-4c50-beaf-dae3f40f932e",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "- Ans: \n",
    "\n",
    "In the context of building a stock price prediction model with a dataset containing numerous features like company financial data and market trends, Principal Component Analysis (PCA) can be employed to reduce the dimensionality of the dataset. Here's a step-by-step explanation:\n",
    "\n",
    "1. Understand the High-Dimensional Data:\n",
    "- Examine the dataset and recognize the high-dimensional nature of the feature space. In stock price prediction, there may be numerous financial indicators, market variables, and other factors contributing to the complexity.\n",
    "\n",
    "2. Standardization:\n",
    "- Standardize the features to ensure they all have zero mean and unit variance. PCA is sensitive to the scale of the features, and standardization helps in giving equal importance to each feature.\n",
    "\n",
    "3. Apply PCA:\n",
    "- Use PCA to transform the standardized dataset into a new set of uncorrelated features, known as principal components. The number of principal components can be determined based on the desired explained variance or a pre-defined number.\n",
    "\n",
    "4. Eigenvalue Decomposition:\n",
    "- PCA involves eigenvalue decomposition of the covariance matrix of the standardized data. The eigenvectors represent the directions of maximum variance, and the eigenvalues indicate the magnitude of variance along those directions.\n",
    "\n",
    "5. Select Principal Components:\n",
    "- Choose the top k eigenvectors corresponding to the highest eigenvalues. This selection determines the number of principal components to retain.\n",
    "\n",
    "6. Project Data:\n",
    "- Project the original high-dimensional data onto the selected principal components to obtain a lower-dimensional representation of the dataset.\n",
    "\n",
    "\n",
    "- Reducing dimensionality through PCA is beneficial in stock price prediction as it simplifies the model, mitigates the curse of dimensionality, and potentially captures the most relevant information for forecasting without using all the original features. The choice of the number of principal components should be based on a balance between computational efficiency and retaining sufficient information for accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a635f0d-6737-422f-a8e0-9c83013a0e03",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "- Ans:\n",
    " the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are approximately [-1, -0.6842, -0.3684, -0.0526, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911a3bb-7b08-4fd4-8fdd-1232d107cfc9",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why.\n",
    "- Ans :\n",
    "The decision on how many principal components to retain in PCA is often based on the explained variance. The explained variance tells us the proportion of the total variance in the original data that is retained by each principal component. A common approach is to retain enough principal components to capture a significant percentage of the total variance, aiming for a balance between dimensionality reduction and information retention.\n",
    "\n",
    "Here's how you can perform feature extraction using PCA and decide on the number of principal components to retain:\n",
    "\n",
    "Standardize the Data:\n",
    "\n",
    "Standardize the features (height, weight, age, blood pressure) to ensure they have zero mean and unit variance. This step is crucial for PCA as it is sensitive to the scale of the features.\n",
    "Apply PCA:\n",
    "\n",
    "Apply PCA to the standardized data.\n",
    "Calculate Explained Variance:\n",
    "\n",
    "Examine the explained variance ratio for each principal component. The explained variance ratio represents the proportion of the dataset's variance that lies along the axis of each principal component.\n",
    "Cumulative Explained Variance:\n",
    "\n",
    "Calculate the cumulative explained variance by summing the explained variance ratios. This will help you understand how much variance is retained as you include more principal components.\n",
    "Decide on the Number of Components:\n",
    "\n",
    "Choose the number of principal components based on the cumulative explained variance. A common threshold is to retain enough components to capture, for example, 95% or 99% of the total variance.\n",
    "\n",
    "Here's an example using Python with scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f58427-64d3-4d5f-8c0c-1b5fa847b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio:\n",
      "[1. 0. 0.]\n",
      "\n",
      "Cumulative Explained Variance:\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is your dataset with columns: height, weight, age, blood pressure\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "data_pca = pca.fit_transform(data_standardized)\n",
    "\n",
    "# Calculate explained variance ratio and cumulative explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Print explained variance and cumulative explained variance\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "print(\"\\nCumulative Explained Variance:\")\n",
    "print(cumulative_explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a61b1-ffb8-468f-bbd6-9af8281228cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
