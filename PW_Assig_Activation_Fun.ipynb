{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Kg1efoqyAfW"
      },
      "outputs": [],
      "source": [
        "# Activation Function Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is an activation function in the context of artificial neural networks?\n",
        "\n",
        "Ans:- An activation function in the context of artificial neural networks is a mathematical function applied to the output of each neuron in a neural network. It introduces non-linearity into the network, allowing it to learn complex patterns in the data.\n",
        "\n",
        "Q2. What are some common types of activation functions used in neural networks?\n",
        "\n",
        "Ans:- Some common types of activation functions used in neural networks include:\n",
        "\n",
        "- Sigmoid function\n",
        "- Hyperbolic tangent (tanh) function\n",
        "- Rectified Linear Unit (ReLU) function\n",
        "- Leaky ReLU function\n",
        "- Softmax function\n",
        "\n",
        "Q3. How do activation functions affect the training process and performance of a neural network?\n",
        "\n",
        "Ans:-  Activation functions affect the training process and performance of a neural network by controlling the output range of each neuron and introducing non-linearity. Proper selection of activation functions can help improve convergence during training and enhance the network's ability to model complex relationships in the data.\n",
        "\n",
        "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
        "\n",
        "Ans:- The sigmoid activation function is a bounded, S-shaped function that squashes the input values between 0 and 1. Its formula is f(x) = 1 / (1 + e^(-x)). Advantages include smooth gradient descent and output interpretation as probabilities. Disadvantages include vanishing gradients for large input values and output values centered around 0.5, causing slow convergence.\n",
        "\n",
        "Q5. What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
        "\n",
        "Ans:- The rectified linear unit (ReLU) activation function is a piecewise linear function that returns the input for positive values and 0 for negative values. It differs from the sigmoid function by being unbounded and not saturating for positive input values.\n",
        "\n",
        "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
        "\n",
        "Ans:- The benefits of using the ReLU activation function over the sigmoid function include faster convergence due to the absence of saturation for positive input values, simpler computation, and reduced likelihood of the vanishing gradient problem.\n",
        "\n",
        "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
        "\n",
        "Ans:- \"Leaky ReLU\" is a variation of the ReLU activation function where a small slope (typically a small positive value) is assigned to negative input values instead of setting them to 0. This addresses the vanishing gradient problem by allowing a small, non-zero gradient for negative inputs, preventing dead neurons during training.\n",
        "\n",
        "Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
        "\n",
        "Ans:- The purpose of the softmax activation function is to convert the raw scores (logits) produced by the output layer of a neural network into probabilities. It is commonly used in multi-class classification problems to produce a probability distribution over multiple classes.\n",
        "\n",
        "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
        "\n",
        "Ans:- The hyperbolic tangent (tanh) activation function is similar to the sigmoid function but outputs values between -1 and 1. It is often preferred over the sigmoid function as it has zero-centered outputs, which may help in faster convergence during training. However, it suffers from the same vanishing gradient problem as the sigmoid function."
      ],
      "metadata": {
        "id": "xgp21-ityUk3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T8nBBPNPy_W4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}