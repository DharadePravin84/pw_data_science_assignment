{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
        "\n",
        "Polynomial functions and kernel functions are both used in the context of machine learning, particularly in Support Vector Machines (SVMs). Here's how they relate:\n",
        "\n",
        "- Polynomial Functions: These are mathematical expressions that involve variables raised to whole number exponents. For example, f(x)=(x^2)+2x+1 is a polynomial function. In machine learning, polynomial features can be used to transform the input data into a higher-dimensional space where it might be easier to separate the classes linearly.\n",
        "\n",
        "- Kernel Functions: These are used in SVMs to enable the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation can be linear or non-linear. A polynomial kernel is a specific type of kernel function that transforms the input data into a higher-dimensional space using polynomial functions. The polynomial kernel function is given by:\n",
        "  K(x,y)=(x⋅y+c)^d\n",
        "  \n",
        "  where d is the degree of the polynomial and c is a constant. This kernel allows the SVM to create non-linear decision boundaries.\n"
      ],
      "metadata": {
        "id": "fUjnm7E3ayKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
      ],
      "metadata": {
        "id": "wmFnuNLdcE1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load a sample dataset\n",
        "data = datasets.load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Preprocess the data (scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create an instance of the SVC classifier with a polynomial kernel\n",
        "svc = SVC(kernel='poly', degree=3, C=1.0, gamma='auto')\n",
        "\n",
        "# Train the classifier on the training data\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the testing data\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpn1iFb1bz-t",
        "outputId": "6e2178c8-0504-430f-f2b4-f9918879faa5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       0.87      1.00      0.93        13\n",
            "           2       1.00      0.85      0.92        13\n",
            "\n",
            "    accuracy                           0.96        45\n",
            "   macro avg       0.96      0.95      0.95        45\n",
            "weighted avg       0.96      0.96      0.96        45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
        "\n",
        "In Support Vector Regression (SVR), the parameter epsilon (ϵ) defines a margin of tolerance where no penalty is given to errors. Increasing the value of ϵ allows more data points to fall within this margin without being penalized, which generally results in fewer support vectors because the model becomes more tolerant to errors."
      ],
      "metadata": {
        "id": "8l8bpEEycVpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
        "\n",
        "- Kernel Function: The kernel function determines the feature space in which the regression is performed. Common kernels include linear, polynomial, and radial basis function (RBF). The choice of kernel depends on the problem. For instance, a linear kernel might be sufficient for linearly separable data, while an RBF kernel might be better for complex, non-linear relationships.\n",
        "\n",
        "- C Parameter: This is the regularization parameter that controls the trade-off between achieving a low error on the training data and minimizing the model complexity (which can lead to overfitting). A large C value will try to classify all training examples correctly, potentially leading to overfitting. A smaller C value will allow some misclassifications but aims for a simpler model.\n",
        "\n",
        "- Epsilon Parameter (ϵ): As mentioned earlier, ϵ defines a margin of tolerance where no penalty is given to errors. A larger ϵ results in fewer support vectors and a smoother model, while a smaller ϵ can capture more detailed patterns but might lead to overfitting.\n",
        "\n",
        "- Gamma Parameter: This parameter defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'. In the case of an RBF kernel, a high gamma value leads to a more complex model that can capture more intricate patterns, potentially leading to overfitting. A lower gamma value results in a simpler model."
      ],
      "metadata": {
        "id": "8gjk566EckcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Assignment:\n",
        "-  Import the necessary libraries and load the dataset\n",
        "- Split the dataset into training and testing sets\n",
        "- Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
        "-  Create an instance of the SVC classifier and train it on the training data\n",
        "- use the trained classifier to predict the labels of the testing data\n",
        "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
        "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance\n",
        "- Train the tuned classifier on the entire dataset\n",
        "- save the trained classifier ti as file for future use.\n"
      ],
      "metadata": {
        "id": "h5exBDIJdGMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pickle\n",
        "\n",
        "# Load the dataset\n",
        "data = datasets.load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Preprocess the data (scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create an instance of the SVC classifier\n",
        "svc = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Use the trained classifier to predict the labels of the testing data\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto']}\n",
        "grid_search = GridSearchCV(SVC(kernel='linear', random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best score: {grid_search.best_score_}\")\n",
        "\n",
        "# Train the tuned classifier on the entire dataset\n",
        "best_svc = grid_search.best_estimator_\n",
        "best_svc.fit(X, y)\n",
        "\n",
        "# Save the trained classifier to a file for future use\n",
        "with open('svc_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump(best_svc, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDEZcqefdGqt",
        "outputId": "0969f5c3-eacc-4338-ee30-a387104ac27c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9777777777777777\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      0.92      0.96        13\n",
            "           2       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n",
            "Best parameters: {'C': 10, 'gamma': 'scale'}\n",
            "Best score: 0.9523809523809523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gb3_WcdWdLIN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}